{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Model Predication with CNN / Aaditya.ipynb",
      "collapsed_sections": [
        "_hMRvgb4W7MF",
        "vuhJ_4N6XCle"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aadityavs/34/blob/main/Model_Predication_with_CNN_Aaditya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVe_ge-0KTVn"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "**Image Dataset Directory Structur**e:\n",
        "\n",
        "**Note: The directory and sub-directory names shown here are only for explanation purposes which might differ from the code.**\n",
        "\n",
        "Suppose if we have a master directory(folder) of the Images then we can subdivide it into “Training”, “Validation” & “Testing” images sub-directories(sub-folder).\n",
        "\n",
        "And then the “Training” directories contain sub-directories(sub-folders) called “Infected” and “Uninfected” which contain appropriate images in the respective sub-directories.\n",
        "\n",
        "Similarly, the “Validation'' & “Testing” directory also contains sub-directories(sub-folders) called “Infected” and “Uninfected” which contain appropriate images in the respective sub-directories.\n",
        "\n",
        "\n",
        "**Training**: Images in this directory will be used for the training of the data.\n",
        "\n",
        "**Validation**: Images in this directory will be used to validate the model training. The validation dataset allows us to see how well the data generalises the classification.\n",
        "\n",
        "**Testing**: Images in this directory will be used to test how well the model is trained.\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/2467514a-e93f-4a0f-8e20-b3893dfa9144.jpeg\" width= 600>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GeLRcJyjNcMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xCYT8gXKTVq",
        "outputId": "670e3c0e-fa0b-4032-ac2f-2c453e029cbb"
      },
      "source": [
        "!git clone https://github.com/procodingclass/PRO-M3-Pneumothorax-Image-Dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PRO-M3-Pneumothorax-Image-Dataset'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (300/300), done.\u001b[K\n",
            "remote: Compressing objects: 100% (297/297), done.\u001b[K\n",
            "remote: Total 313 (delta 3), reused 300 (delta 3), pack-reused 13\u001b[K\n",
            "Receiving objects: 100% (313/313), 118.60 MiB | 23.15 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (602/602), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYcwxDFtyhEH"
      },
      "source": [
        "## Image Data Preprocessing:\n",
        "1. Convert each image to an array\n",
        "2. Map each image labels\n",
        "3. Augment the each image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtXxDq1O4okc"
      },
      "source": [
        "### Image Preprocessing: Mapping each image with labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DIAsjUd343_"
      },
      "source": [
        "<center><b>Mapping Each Image With Labels</b><br><img src=\"https://s3-whjr-curriculum-uploads.whjr.online/1bdade80-2a32-4fc2-8902-f18067803dba.jpeg\" width= 1000>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cHrlB-5xQ4"
      },
      "source": [
        "### Image Preprocessing: Data Augmentation\n",
        "\n",
        "A few Data Augemtation Techniques:\n",
        "\n",
        "*   Image Rotation\n",
        "*   Image Height & Width Shift\n",
        "*   Image Horizontal & Vertical Flipping\n",
        "*   Image Resizing\n",
        "*   Image Zooming\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/5403e9b1-a339-405d-98b3-36826ec3f04a.gif\" width= 400>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NhJWjpUVBpu"
      },
      "source": [
        "#### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5-_nrO7yn2D",
        "outputId": "b1752bc6-d857-4641-dda1-5c2212645ab4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "\n",
        "# Random Data Augmentation(Rescale, Rotation, Flips, Zoom, Shifts) using ImageDataGenerator\n",
        "training_data_generator = ImageDataGenerator(\n",
        "    rescale = 1.0/255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n",
        "# Image Directory\n",
        "training_image_directory = \"/content/PRO-M3-Pneumothorax-Image-Dataset/training_dataset\"\n",
        "\n",
        "# Generate Preprocessed Augmented Data\n",
        "training_augmented_images = training_data_generator.flow_from_directory(\n",
        "    training_image_directory,\n",
        "    target_size=(180,180))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOaQFdFVGmT"
      },
      "source": [
        "#### Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS-Jx1EB-OFg",
        "outputId": "e76cd4b2-92a1-4d68-e59d-c44094ca7bef"
      },
      "source": [
        "# Random Data Augmentation(Rescale) using ImageDataGenerator\n",
        "validation_data_generator = ImageDataGenerator(rescale = 1.0/255)\n",
        "\n",
        "# Image Directory\n",
        "validation_image_directory = \"/content/PRO-M3-Pneumothorax-Image-Dataset/validation_dataset\"\n",
        "\n",
        "# Generate Preprocessed Augmented Data\n",
        "validation_augmented_images = validation_data_generator.flow_from_directory(\n",
        "    validation_image_directory,\n",
        "    target_size=(180,180))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivutzYmWlXBb"
      },
      "source": [
        "#### Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUU7UKNhyftg",
        "outputId": "ba4b37ce-3214-4d8c-fd8b-5b2c0207c105"
      },
      "source": [
        "training_augmented_images.class_indices"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'infected': 0, 'uninfected': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKguVEnx3O4m"
      },
      "source": [
        "## Convolutional Neural Network Architecture\n",
        "A CNN model have:\n",
        "\n",
        "1. **Feature Learning layers**:\n",
        "\n",
        "   1.1 Convolution + Activation(RELU) layers\n",
        "\n",
        "   1.2 Pooling layers\n",
        "\n",
        "2. **Classification layers**:\n",
        "\n",
        "   2.1 Flatten layer\n",
        "\n",
        "   2.2 Fully connected(Dense) layer\n",
        "\n",
        "   2.3 Fully connected(Dense) layer with Softmax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/9d09af47-3a9d-48b7-a05c-8941009442ea.png\" width= 1500>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T-8YEZK3aj_"
      },
      "source": [
        "**Feature Extraction Visualisation(Convolution + Relu)**\n",
        "\n",
        "The convolution is a mathematical computation between two arrays, the image array and the filter array which gives a new image array.\n",
        "\n",
        "\n",
        "Visually we can understand that the feature detector/filter moves over the image to extract features from the image.\n",
        "\n",
        "\n",
        "\n",
        "[<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/3917c089-1d8f-4f32-b5c4-401b6abe8d47.gif\" width= 500>](https://)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHHUNrcWC91w"
      },
      "source": [
        "## Mathematically:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQ9L7AzCXN3"
      },
      "source": [
        "**Conv2D Layer**\n",
        "\n",
        "The convolution is a mathematical computation between two 2D arrays, the image array and the filter array which gives a new image array.\n",
        "\n",
        "A portion of the input image array matrix, called a sub-array(size same as the size of the filter) is taken, starting from the top left.\n",
        "\n",
        "This sub-array is multiplied with the filter array. We can multiply one array matrix to another, by multiplying 1st element to 1st element of both the arrays(2nd element to 2nd element of both arrays and so on).\n",
        "\n",
        "After multiplying the result is added, which gives the value of the 1st element of the new image array.\n",
        "\n",
        "Then we shift towards right by one column, repeat the above steps to get the value of the 2nd element of the new array.\n",
        "\n",
        "Once the whole row is finished we shift downwards by one row, repeat the above steps to get the value of all elements of the new array one by one.\n",
        "\n",
        "The whole process is repeated with different filters, to get different output, which all together is the output of the 1st Conv2D layer.\n",
        "\n",
        "These outputs from the 1st Conv2D layer are given to the 2nd Conv2D layer and convolutions are performed.\n",
        "\n",
        "This repeated for all the layers of the CNN model.\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/306591ba-2163-45d9-9c60-dbc895332982.gif\" width= 800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZhK3L6qCW0C"
      },
      "source": [
        "**ReLU**\n",
        "\n",
        "ReLU is defined as a function, y= f(x) such that it gives x for all values of x > 0 and 0 for all values of x<0.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/6986c680-7ad3-4fad-bf04-59cecfa5e0e3.png\" width= 600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yV1sDPWBZsS"
      },
      "source": [
        "**MaxPooling2D**\n",
        "\n",
        "First there is an input array(for example 4x4) and pool size(for example 2x2). Pool size is always smaller than the input array size.\n",
        "\n",
        "Then the maximum value is taken from the sub-array of size equal to pool size.\n",
        "\n",
        "The result after applying the Max Pooling will be the new array of size equal to the half of the size of the original input array.\n",
        "\n",
        "Since our input array is 4x4, after max pooling the new array will be 2x2, hence reducing the dimension of the array.\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/d485cb40-4db6-4fb6-9051-d9e888883053.jpg\" width= 800>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hMRvgb4W7MF"
      },
      "source": [
        "## Define/Build Convolution Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model=tf.keras.models.Sequential([\n",
        "    #1st convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu' , input_shape=(180,180,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #2nd convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #3rd convolation\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #4th convolution\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flatten\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #Dropout\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    #Dense\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "lh8qMK6qdlUQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_n5TpnIgNfT",
        "outputId": "2a2ec704-c2bc-4dcb-fa0b-c7ad28b5f0cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 178, 178, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 89, 89, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 87, 87, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 43, 43, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 41, 41, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 20, 20, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 18, 18, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 9, 9, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 5184)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5184)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               2654720   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2768322 (10.56 MB)\n",
            "Trainable params: 2768322 (10.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOw4OJQo-p-9"
      },
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential([\n",
        "\n",
        "    # 1st Convolution & Pooling layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(180, 180, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    # 2nd Convolution & Pooling layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # 3rd Convolution & Pooling layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # 4th Convolution & Pooling layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flatten the results to feed into a Dense Layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    # Classification Layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhJ_4N6XCle"
      },
      "source": [
        "## Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvxQyRT2-wBY",
        "outputId": "673a6a17-01d3-40c1-ea02-aa187666afd4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 178, 178, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 89, 89, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 87, 87, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 43, 43, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 41, 41, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 20, 20, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 18, 18, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 9, 9, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 5184)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5184)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               2654720   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2768322 (10.56 MB)\n",
            "Trainable params: 2768322 (10.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U9aDI1zWEs6"
      },
      "source": [
        "## Visualize convolutional layers intermediate ouputs\n",
        "\n",
        "\n",
        "This visual shows how the features are extracted layer after layer in **4** **convolutional layers** (conv2d, conv2d_1, conv2d_2, conv2d_3) with **max pooling layers**(max_pooling2d, max_pooling2d_1, max_pooling2d_2, max_pooling2d_3)\n",
        "\n",
        "**Note: This visual is created only for explanation purposes.**\n",
        "\n",
        "At the start of a convolutional network, the filter(feature detector/kernel) detects simple patterns, like horizontal lines, vertical lines, and corners, simple shapes.\n",
        "\n",
        "In later layers of the network, filters(feature detector/kernel) are complex that detect shapes, objects, and other complex structures, which is done by using the previously generated feature and their detected simple features is used to build more complex ones.\n",
        "\n",
        "\n",
        "*Note: As we go deeper in the layers, the feature becomes increasingly complex hence less visually interpretable. They begin to encode higher-level concepts such as single borders, corners and angles. Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image, this why the outputs of the Dense Layer will not be shown for visual explanation.*\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/b941c8bd-c137-449f-ae8f-7233735a7845.jpg\" width= 1200>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIxfzpbqXSZJ"
      },
      "source": [
        "## Compile Model\n",
        "\n",
        "Before training the model we need to compile it. We compile the model using the **compile()** method(Keras).\n",
        "\n",
        "The compile method takes many arguments, but we will pass the three arguments which must be specified. The arguments are:\n",
        "\n",
        "1. Optimizers\n",
        "\n",
        "2. Loss function\n",
        "\n",
        "3. Metrics for prediction\n",
        "\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/239bdfe5-057d-41ae-86a9-a73ce92c9998.png\" width= 500>\n",
        "\n",
        "**Need of Model Compilation:**\n",
        "\n",
        "When the model is trained it can, almost never, be 100% efficient, that it cannot always predict the class of the image correctly.\n",
        "\n",
        "This leads to the concept of loss during model training, which tells us how bad the model is performing.\n",
        "\n",
        "Hence we need to use the loss functions(these are mathematical computation functions) to get the value of the loss.\n",
        "\n",
        "For example, the result of the loss function gives the value as 0.45, this means that 45 %(0.45X100) of the times, the model will predict wrong results, and only 55% times will predict the correct results!\n",
        "\n",
        "That means, we should try to minimize the loss function value, because a lower loss value means our model is going to perform better. The process of minimizing (or maximizing) the value of a mathematical function/expression is called optimization.\n",
        "\n",
        "<br><img src=\"https://s3-whjr-curriculum-uploads.whjr.online/655fb95e-40b3-4bc1-988b-21ba782bf54d.png\" width= 400>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qDoACDy-zpl"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9xcaotqXX9y"
      },
      "source": [
        "## Fit & Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_wpcdoL-5Lb"
      },
      "source": [
        "history = model.fit(training_augmented_images, epochs=20, validation_data = validation_augmented_images, verbose=True)\n",
        "\n",
        "model.save(\"Pneumothorax.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "j0pkIulRiWXm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(training_augmented_images, epochs=20, validation_data = validation_augmented_images, verbose = True)\n",
        "model.save('Pneumothorax.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE__djwfikdI",
        "outputId": "6bc0db57-3428-4a49-e1ac-0691cd83622e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.4850\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6938 - accuracy: 0.4650 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6929 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 40s 6s/step - loss: 0.6923 - accuracy: 0.5400 - val_loss: 0.6929 - val_accuracy: 0.4950\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6946 - accuracy: 0.5200 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 30s 5s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 33s 5s/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 34s 5s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 32s 5s/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}